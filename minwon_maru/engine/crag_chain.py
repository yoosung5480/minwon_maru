# basic_CRAG.py  (chain.py + basic_CRAG.py í†µí•© / search_document CRAG ë‚´ë¶€ ì™„ì „ í†µí•©)
from pathlib import Path
import json
from typing import Annotated, List, Any, Optional
from typing_extensions import TypedDict

# ===== chain.py ì— ìˆë˜ import =====
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
# from langchain_openai import ChatOpenAI  # (ì›ë³¸ ì£¼ì„ ìœ ì§€)

from minwon_maru.tools.context import create_metadata_retriever_with_map, create_workpages_retriever
from minwon_maru.tools.llms import llm_list
from minwon_maru.tools.personal_info_keeper import personal_info_keeper
import asyncio

from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_openai import OpenAIEmbeddings  # (ì›ë³¸ ì£¼ì„ ìœ ì§€)

# ===== basic_CRAG.py ì— ìˆë˜ import =====
from langgraph.graph import END, StateGraph, START
from minwon_maru.prompts import prompt
from minwon_maru.tools import llms
from langchain_teddynote.tools.tavily import TavilySearch
from pydantic import BaseModel, Field


#########################################################################################################
# 1) í”„ë¡¬í”„íŠ¸ ì •ì˜ (chain.py ì›ë³¸ ìœ ì§€)
#########################################################################################################
prompt_tpl = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ë‹¹ì‹ ì€ {ability}ì— ëŠ¥ìˆ™í•œ ì¹œì ˆí•œ ë¯¼ì› ì‘ëŒ€ ìƒë‹´ì›ì…ë‹ˆë‹¤. "
            "ì•„ë˜ ë¬¸ì„œ ë§¥ë½(Context)ì„ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•´ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. "
            "ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.\n\n"
            "[Context]\n{context}"
        ),
        MessagesPlaceholder("history"),
        ("human", "{input}"),
    ]
)


# ===========================
# Grader Definition
# ===========================
class GradeDocuments(BaseModel):
    binary_score: str = Field(description="Documents are relevant to the question, 'yes' or 'no'")


#########################################################################################################
# basic_CRAG.py ì›ë³¸ íƒ€ì…
#########################################################################################################
class GraphState(TypedDict):
    input: Annotated[str, "The question text from user"]
    ability: Annotated[str, "The domain ability (ì˜ˆ: ë¯¼ì› í–‰ì •, ìˆ˜í•™ ë“±)"]
    generation: Annotated[str, "The generation from the LLM"]
    web_search: Annotated[str, "Whether to add search (Yes/No)"]
    relavent_workpages : Annotated[str, "relavent workpages link info"]
    context : Annotated[str, "relavent documents info"]
    documents: Annotated[Optional[List[Document]], "Documents already retrieved (may be empty or None)"]

class Workflow:
    def ainvoke(self, question: str) -> str: ...


#########################################################################################################
# CRAG (search_document ë‚´ë¶€ í†µí•© + ì„¸ì…˜ store í•„ë“œ ë³´ìœ )
#########################################################################################################
class CRAG(Workflow):
    def __init__(
        self,
        rag_pipeline,
        *,  # ëª…ì‹œì  í‚¤ì›Œë“œ ì¸ì
        metadata_retriever,
        summarize_map,
        workpage_retriever,
        llm_list=llms.llm_list,
    ):  
        self.chat_count = 0
        self.raw_inputs = []
        self.masked_inputs = []
        self.llm_outputs = []
        self.llm_list = llm_list
        self.rag_pipeline = rag_pipeline
        self.web_search_prompt = prompt.prompt_to_refine_text

        # í‰ê°€ì ì •ì˜
        self.structured_llm_grader = self.llm_list["gpt-4.1-mini"].with_structured_output(GradeDocuments)

        # ğŸ”½ ì¶”ê°€: ë¬¸ì„œ-ì§ˆë¬¸ ê´€ë ¨ì„± ê·¸ë ˆì´ë” í”„ë¡¬í”„íŠ¸ + ì²´ì¸
        grader_prompt = ChatPromptTemplate.from_messages([
            ("system",
             "You are a strict retrieval evaluator. "
             "Given a user question and a document chunk, decide if the document helps answer the question. "
             "Answer 'yes' only if the document contains facts, procedures, definitions, hours, numbers, or rules that are directly relevant. "
             "Otherwise answer 'no'. Output 'binary_score' as 'yes' or 'no'."),
            ("human", "Question:\n{question}\n\nDocument:\n{document}")
        ])
        self.retrieval_grader = grader_prompt | self.structured_llm_grader

        # ì„¸ì…˜ íˆìŠ¤í† ë¦¬ storeë¥¼ CRAG í•„ë“œë¡œ ë³´ìœ 
        self.store: dict[str, ChatMessageHistory] = {}
        self.question_history = []

        # chain.pyì—ì„œ ê°€ì ¸ì˜¤ë˜ ë¦¬íŠ¸ë¦¬ë²„/ë§µì„ CRAG í•„ë“œë¡œ ë³´ê´€
        self.metadata_retriever = metadata_retriever
        self.summarize_map = summarize_map
        self.workpage_retriever = workpage_retriever

        self.web_search_tool = TavilySearch(max_results=3)
        self.work_chain = self.define_workflow()

    # [ì¶”ê°€] CRAG ì¸ìŠ¤í„´ìŠ¤ì˜ ì„¸ì…˜ íˆìŠ¤í† ë¦¬ getter
    def get_session_history(self, session_id: str) -> BaseChatMessageHistory:
        if session_id not in self.store:
            self.store[session_id] = ChatMessageHistory()
        return self.store[session_id]

    # ==================== ë…¸ë“œ í•¨ìˆ˜ ====================
    def search_document(self, state: GraphState):
        """
        (í†µí•©) chain.pyì˜ search_document ë¡œì§ì„ CRAG ë‚´ë¶€ë¡œ ì™„ì „ í†µí•©.
        stateì— documents/context/doc_retrieverë¥¼ ì±„ì›Œì„œ ë°˜í™˜.
        """
        query = state["input"]
        outputs = dict(state)

        # 1. ì§ˆì˜ì™€ ìœ ì‚¬í•œ ìƒìœ„ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        candidate_docs = self.metadata_retriever.get_relevant_documents(query)

        # 2. ì§ˆì˜ì™€ ìœ ì‚¬í•œ workpage ê²€ìƒ‰
        selected_work = self.workpage_retriever.get_relevant_documents(query)

        # 3. ì„ íƒëœ ë¬¸ì„œë¥¼ chunkë¡œ ë¶„í• 
        split_documents = []
        for d in candidate_docs:
            origin_info = self.summarize_map.get(d.page_content, {})
            json_path = origin_info.get("path")
            if json_path:
                with open(json_path, "r", encoding="utf-8") as f:
                    origin_json = json.load(f)
                if isinstance(origin_json, list) and len(origin_json) > 0:
                    content = origin_json[0].get("page_content", "")
                else:
                    content = origin_json.get("content", "") if isinstance(origin_json, dict) else ""
            else:
                content = d.page_content

            splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
                encoding_name="cl100k_base",
                chunk_size=300,   
                chunk_overlap=50,
                separators=["\n### ","\n## ","\n# ","\n\n","\n"," ",""]
            )
            base_doc = Document(page_content=content, metadata={"path": json_path})
            split_documents.extend(splitter.split_documents([base_doc]))

        # 4. ë²¡í„°ìŠ¤í† ì–´ ìƒì„± í›„, queryì™€ 0.5 ì´ìƒ ë¬¸ì„œ chunk ê²€ìƒ‰
        if split_documents:
            vectorstore = FAISS.from_documents(
                split_documents,
                embedding=self.metadata_retriever.vectorstore.embedding_function
            )
            doc_retriever = vectorstore.as_retriever(
                search_type="similarity_score_threshold",
                search_kwargs={"score_threshold": 0.5, "k": 8}
            )
            retrieved_chunks = doc_retriever.get_relevant_documents(query)
        else:
            doc_retriever = None
            retrieved_chunks = []

        # context ì¡°í•©
        context_parts=[]
        workpath_context_parts = []
        if selected_work:
            workpath_context_parts.append("[í–‰ì • Work Pages]")
            for w in selected_work:
                url = w.metadata.get("url") or w.metadata.get("page_link", "")
                desc = w.metadata.get("desc", "")
                summarize = w.page_content
                workpath_context_parts.append(f"- {desc or summarize} | {url}")

        if retrieved_chunks:
            context_parts.append("[ê´€ë ¨ ë¬¸ì„œ]")
            for c in retrieved_chunks:
                context_parts.append(c.page_content)

        context_str = "\n".join(context_parts)
        workpath_context_str = "\n".join(workpath_context_parts)
        outputs["relavent_workpages"] = workpath_context_str

        # ê²°ê³¼ ì£¼ì… 
        outputs["documents"] = retrieved_chunks
        # print("===============context===============")
        # print(context_str)
        outputs["context"] = context_str
        outputs["doc_retriever"] = doc_retriever
        return outputs

    # def grade_documents(self, state: GraphState):
    #     """
    #     search_documentê°€ ë§Œë“¤ì–´ ë‘” documents(List[Document])ê°€
    #     ì¡´ì¬í•˜ê³  ê¸¸ì´ê°€ >0 ì´ë©´ ë°”ë¡œ generate, ì•„ë‹ˆë©´ web_search.
    #     """
    #     print("\n==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====\n")
    #     docs = state.get("documents")
    #     print(docs)
    #     has_docs = bool(docs) and len(docs) > 0
    #     result = {"web_search": "No" if has_docs else "Yes"}
    #     print(result)
    #     return result
    
    def grade_documents(self, state: GraphState):
        print("\n==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====\n")
        question = state.get("input", "")
        documents = state.get("documents") or []

        # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì›¹ì„œì¹˜ë¡œ ì „í™˜
        if not documents:
            print("==== [GRADE: NO DOCUMENTS] ====")
            return {"documents": [], "web_search": "Yes"}

        filtered_docs = []
        for d in documents:
            doc_text = (d.page_content or "")  # ê³¼ë„í•œ í† í° ë°©ì§€ë¡œ íŠ¸ë ì¼€ì´íŠ¸
            try:
                grade = self.retrieval_grader.invoke({"question": question, "document": doc_text})
                keep = (grade.binary_score or "").strip().lower() == "yes"
            except Exception as e:
                print(f"==== [GRADE: ERROR] ==== {e}")
                # ì—ëŸ¬ ì‹œ ë³´ìˆ˜ì ìœ¼ë¡œ ìœ ì§€(ë§ì‹¤ ë°©ì§€)
                keep = True

            if keep:
                print("==== [GRADE: DOCUMENT RELEVANT] ====")
                filtered_docs.append(d)
            else:
                print("==== [GRADE: DOCUMENT NOT RELEVANT] ====")

        # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì›¹ì„œì¹˜ë¡œ ì „í™˜ -> filtered_docsê°€ ì—†ì„ë•Œë„ ì „í™˜.
        # contextê°€ ì—†ì´ ì œê³µë˜ëŠ” ìƒí™©ì„ ë°©ì§€í•œë‹¤. ì–´ë–¤ì¼ì´ ìˆì–´ì„œ contextëŠ” ì›¹ì„œì¹˜ ê¸°ë°˜ì´ë¼ë„ ë°˜í™˜í•˜ê²Œ í•´ì•¼í˜
        if not documents or not filtered_docs:
            print("==== [GRADE: NO DOCUMENTS] ====")
            return {"documents": [], "web_search": "Yes"}
        
        # ê¸°ì¡´ contextì—ì„œ [ê´€ë ¨ ë¬¸ì„œ] ì„¹ì…˜ë§Œ êµì²´í•˜ì—¬ ìœ ì§€(Work PagesëŠ” ë³´ì¡´)
        original_context = state.get("context", "")
        before, sep, _after = original_context.partition("[ê´€ë ¨ ë¬¸ì„œ]")

        new_context_parts = [before.strip()] if before.strip() else []
        if filtered_docs:
            new_context_parts.append("[ê´€ë ¨ ë¬¸ì„œ]")
            for c in filtered_docs:
                new_context_parts.append(c.page_content)

        new_context = ""
        if state["relavent_workpages"] :
            new_context  = "[ê´€ë ¨ ë§í¬]\n"+(state["relavent_workpages"])
        new_context = "\n".join(new_context_parts)
        print("new context \n", new_context)
        web_search_flag = "No" if filtered_docs else "Yes"
        state["documents"] = filtered_docs
        state["web_search"] = web_search_flag
        state["context"] = new_context
        return state


    def web_search(self, state: GraphState):
        print("\n==== [WEB SEARCH] ====\n")
        query_text, ability = state["input"], state["ability"]
        history = state.get("history", [])
        raw_input = state.get("raw_input", query_text)  # â† ì¶”ê°€

        docs = self.web_search_tool.invoke({"query": query_text})
        context = docs[0] if docs else self.llm_list["gpt-4.1-mini"].invoke(query_text)
        print("web search result : \n", context)
        messages = self.web_search_prompt.format(context=context, question=query_text)

        generation = self.rag_pipeline.invoke({
            "input": messages,
            "raw_input": raw_input,      # â† ì¶”ê°€
            "ability": ability,
            "history": history,
            "context": ""                # ì›¹ì„œì¹˜ ë¸Œë¦¿ì§€ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ë˜ì§€ëŠ” ê²½ìš°
        })
        return {"generation": generation}


    def generate(self, state: GraphState):
        print("\n==== GENERATE ====\n")
        query_text, ability = state["input"], state["ability"]
        print("======query_text======")
        print(query_text)
        history = state.get("history", [])
        context = state.get("context", "")
        print("======context======")
        print(context)
        raw_input = state.get("raw_input", query_text)  # â† ì¶”ê°€

        generation = self.rag_pipeline.invoke({
            "input": query_text,
            "raw_input": raw_input,      # â† ì¶”ê°€: íˆìŠ¤í† ë¦¬ ì €ì¥ìš© ì›ë³¸
            "ability": ability,
            "history": history,
            "context": context
        })
        return {"generation": generation}


    def passthrough(self, state: GraphState):
        print("==== [PASS THROUGH] ====")
        print("Final Answer:", state["generation"])
        return {"generation": state["generation"]}

    def decide_to_generate(self, state: GraphState):
        print("==== [ASSESS GRADED DOCUMENTS] ====")
        return "web_search_node" if state["web_search"] == "Yes" else "generate"

    # ==================== ê·¸ë˜í”„ ì •ì˜ ====================
    def define_workflow(self):
        workflow = StateGraph(GraphState)

        workflow.add_node("search_document", self.search_document)   # â† ë¨¼ì € ê²€ìƒ‰
        workflow.add_node("grade_documents", self.grade_documents)
        workflow.add_node("generate", self.generate)
        workflow.add_node("web_search_node", self.web_search)
        workflow.add_node("pass", self.passthrough)

        workflow.add_edge(START, "search_document")
        workflow.add_edge("search_document", "grade_documents")
        workflow.add_conditional_edges(
            "grade_documents",
            self.decide_to_generate,
            {
                "web_search_node": "web_search_node",
                "generate": "generate",
            },
        )
        workflow.add_edge("web_search_node", "pass")
        workflow.add_edge("generate", "pass")   # ëˆ„ë½ëë˜ ì—£ì§€ ì¶”ê°€
        workflow.add_edge("pass", END)

        return workflow.compile()


    # ==================== ì§„ì…ì  ====================
    # CRAG.invoke ë‚´ë¶€ ìˆ˜ì •
    def invoke(self, question: Any, config: Optional[dict] = None) -> str:
        """
        question ì˜ˆ:
        {
        "input": "...",
        "ability": "...",
        "documents": [...],
        "history": [...]
        }
        """
        # 1) ì›ë³¸/ë§ˆìŠ¤í‚¹ ë¶„ë¦¬ (ì²´ì¸ ì‹¤í–‰ ì „)
        masked_dict, raw_dict = personal_info_keeper(question)
        raw_text = raw_dict.get("input", "")
        masked_text = masked_dict.get("input", "")

        # 2) work_chainìš© ì´ˆê¸° state êµ¬ì„±:
        #    - LLMì—ëŠ” masked input ì‚¬ìš© (state["input"])
        #    - ì„¸ì…˜ íˆìŠ¤í† ë¦¬ì—ëŠ” raw_input ì €ì¥ (RunnableWithMessageHistory ì—ì„œ ì‚¬ìš©)
        state = {
            **question,
            "input": masked_text,                 # LLMì— ë“¤ì–´ê°ˆ ë§ˆìŠ¤í‚¹ ì…ë ¥
            "raw_input": raw_text,                # íˆìŠ¤í† ë¦¬ì— ì €ì¥í•  ì›ë³¸ ì…ë ¥
            "ability": question.get("ability", "ì¼ë°˜ ì§€ì‹"),
            "history": question.get("history", []),
            "documents": question.get("documents"),
        }
        
        result = self.work_chain.invoke(state, config=config)
        
        self.raw_inputs.append(raw_text)
        self.masked_inputs.append(masked_text)
        self.llm_outputs.append(result)
        self.chat_count += 1
        return result



#########################################################################################################
# ë‹µë³€ ì²´ì¸ ìƒì„±. (chain.py ì˜ get_chain â†’ í†µí•©)
# - ìš”êµ¬ì‚¬í•­:
#   1) ì±„íŒ… íˆìŠ¤í† ë¦¬ ë„˜ê¸°ê¸°
#   2) ì„¸ì…˜ì—ëŠ” "ì›ë³¸(raw) input"ë§Œ ê¸°ë¡, LLMì—ëŠ” personal_info_keeperë¡œ ë§ˆìŠ¤í‚¹ëœ input ì‚¬ìš©
#########################################################################################################
def get_chain(
    metadata_path: Path,
    workpages_path: Path,
    embeddings,
) -> CRAG:
    # 1) retriever ìƒì„±
    metadata_retriever, summarize_map = create_metadata_retriever_with_map(
        metadata_path,
        embeddings,
    )
    workpage_retriever = create_workpages_retriever(
        workpages_path,
        embeddings,
    )

    # 2) RAG ì²´ì¸ (ì‘ë‹µ ë¬¸ìì—´ ìƒì„±ìš© ê·¸ëŒ€ë¡œ ìœ ì§€)
    RAGchain = (
    {
        # context: search_documentê°€ ë§Œë“  ê²ƒì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë¬¸ìì—´/ë¦¬ìŠ¤íŠ¸ ëª¨ë‘ ì•ˆì „ ì²˜ë¦¬)
        "context": RunnableLambda(lambda x: (
            "\n".join(
                [d.page_content if isinstance(d, Document) else str(d)]
                for d in x["context"]
            ) if isinstance(x.get("context"), list)
            else x.get("context", "")
        )),
        "documents": RunnableLambda(lambda x: x.get("documents", [])),
        "ability": RunnableLambda(lambda x: x.get("ability", "ì¼ë°˜ ì§€ì‹")),
        # input: ë°˜ë“œì‹œ ë¬¸ìì—´ë§Œ ë“¤ì–´ê°€ê²Œ
        "input": RunnableLambda(lambda x: x.get("input", "")),
        "history": RunnableLambda(lambda x: x.get("history", [])),
    }
    | prompt_tpl
    | llm_list["gpt-4.1"]
    | StrOutputParser()
)

    # 3) CRAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (store í•„ë“œ ë³´ìœ , ë¦¬íŠ¸ë¦¬ë²„ ë³´ê´€)
    crag_chain = CRAG(
        rag_pipeline=None,  # ì•„ë˜ì—ì„œ with_message_history ì£¼ì…
        metadata_retriever=metadata_retriever,
        summarize_map=summarize_map,
        workpage_retriever=workpage_retriever,
    )

    # 4) personal_info_keeper â†’ RAG
    #    - LLMì—ëŠ” ë§ˆìŠ¤í‚¹ëœ inputë§Œ ì‚¬ìš©
    #    - ì„¸ì…˜ íˆìŠ¤í† ë¦¬ì—ëŠ” "ì›ë³¸(raw) input"ë§Œ ê¸°ë¡
    #    => personal_info_keeper ê²°ê³¼ (masked_dict, raw_str)ë¥¼ ë°›ì•„
    #       masked_dictì— raw_input í‚¤ë¥¼ ì¶”ê°€í•´ì„œ ë°˜í™˜
    # ìˆ˜ì •
    def _mask_only_keep_others(x):
        masked, _raw = personal_info_keeper(x)
        # ë‚˜ë¨¸ì§€ í‚¤ ë³´ì¡´ + inputë§Œ ë§ˆìŠ¤í‚¹ ê°’ìœ¼ë¡œ êµì²´
        return {**x, "input": masked.get("input", x.get("input", ""))}

    personal_info_keeper_r = RunnableLambda(_mask_only_keep_others)

    full_chain = personal_info_keeper_r | RAGchain  # LLMì—ëŠ” ë§ˆìŠ¤í‚¹ëœ inputë§Œ ì „ë‹¬ë¨

    # 5) ì„¸ì…˜ íˆìŠ¤í† ë¦¬ê¹Œì§€ ë¶™ì´ê¸°
    #    - input_messages_key="raw_input" ìœ¼ë¡œ ì„¤ì •í•˜ì—¬
    #      ì„¸ì…˜ì—ëŠ” "ì›ë³¸ ì…ë ¥"ë§Œ ì €ì¥ë˜ë„ë¡ í•¨
    with_message_history = RunnableWithMessageHistory(
        full_chain,                         # ê¸°ì¡´ê³¼ ë™ì¼
        crag_chain.get_session_history,     # CRAG í•„ë“œ(store) ì‚¬ìš©
        input_messages_key="raw_input",     # â˜… ì›ë³¸ inputë§Œ ì„¸ì…˜ì— ì €ì¥
        history_messages_key="history",
    )


    # 6) CRAGì— rag_pipeline ì£¼ì…
    crag_chain.rag_pipeline = with_message_history

    return crag_chain
